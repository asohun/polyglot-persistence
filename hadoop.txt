sudo netstat -lptu

1) Set hostname 
vi /etc/hosts

2) Remove language warnings, if they occur
export LANGUAGE=en_US.UTF-8
export LANG=en_US.UTF-8
export LC_ALL=en_US.UTF-8
locale-gen en_US.UTF-8
dpkg-reconfigure locales

3) Configure JRE
Downlaod and extract JRE
cd /usr/local
sudo tar xzf jre.tar.gz
sudo ln -s jre java
Set /etc/profile
export JAVA_HOME
export PATH=$PATH:$JAVA_HOME/bin

4) Create group and user
sudo addgroup hadoop
sudo adduser --ingroup hadoop hduser

5) Install requirements
sudo apt-get install ssh

6) Configure system for installation
su - hduser
ssh-keygen -t rsa -P ‘’
cat $HOME/.ssh/id_rsa.pub >> $HOME/.ssh/authorized_keys
ssh localhost & exit to test

/etc/sysctl.conf >
# disable ipv6
net.ipv6.conf.all.disable_ipv6 = 1
net.ipv6.conf.default.disable_ipv6 = 1
net.ipv6.conf.lo.disable_ipv6 = 1

Reboot

cat /proc/sys/net/ipv6/conf/all/disable_ipv6

7) Install Hadoop
Download and extract Hadoop
cd /usr/local
sudo tar xzf hadoop-1.0.3.tar.gz
sudo mv hadoop-1.0.3 hadoop
sudo chown -R hduser:hadoop hadoop

Set bashrc
# Set Hadoop-related environment variables
export HADOOP_HOME=/usr/local/hadoop

# Set JAVA_HOME
export JAVA_HOME=/usr/lib/jvm/java-6-sun

# Some convenient aliases and functions for running Hadoop-related commands
unalias fs &> /dev/null
alias fs="hadoop fs"
unalias hls &> /dev/null
alias hls="fs -ls"

# If you have LZO compression enabled in your Hadoop cluster and
# compress job outputs with LZOP (not covered in this tutorial):
# Conveniently inspect an LZOP compressed file from the command
# line; run via:
#
# $ lzohead /hdfs/path/to/lzop/compressed/file.lzo
#
# Requires installed 'lzop' command.
#
lzohead () {
    hadoop fs -cat $1 | lzop -dc | head -1000 | less
}

# Add Hadoop bin/ directory to PATH
export PATH=$PATH:$HADOOP_HOME/bin

8) Set JAVA_HOME in Hadoop: /etc/hadoop/hadoop-env.sh

9)  /usr/local/hadoop/sbin/start-all.sh

NOTES
1) You need to make sure the core-site.xml is serving to 0.0.0.0 instead of 127.0.0.1(localhost).
If you get the EOF exception, it means that the port is not accessible externally on that ip, so there is no data to read between the hadoop client / server ipc.

2) Add user accessing HDFS
groupadd supergroup
usermod -a -G supergroup spry 

3) hadoop namenode -format


-> chk how to fix port of datanode

